LwF：

基于正则化的方法
使用蒸馏的方法保留旧有知识。theta_s, theta_o, theta_n分别表示特征层，旧的分类头，新的头

训练时候分为两步：
1，warmup，只训练n，直到收敛；
2，将原有模型的logits，经过温度T的蒸馏以后作为对o的监督信息，同时用grandtruth作为n的监督信息，
s解冻，做联合训练



iCaRL：

基于重演的方法
设定一个固定大小集合用于保存旧的训练样例
分类头是最邻近分类头，直接比较输出logits和各个原型向量的距离
训练时分为三步：
1，用分类损失（CE）和蒸馏损失对模型进行更新
2，去除保存的旧类别中的多余样例
3，构建新类别的保存样例，按照距离类原型的距离作为有限度

因为保留了完整的样例而不是蒸馏logits，故效果比LwF更好


这两种方法都是直接进行端到端的蒸馏，如果对特征层做蒸馏效果会不会更好？



