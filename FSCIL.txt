TOPIC：
Few-Shot Class-Incremental Learning

提出了可以用neural gas network去学习特征向量的拓扑结构，其实有点像原形网络，区别是原型网络只对结果有约束，但是他
对于特征空间的结构进行约束，相当于进行了更高层面上的约束

遗忘问题：使用了锚点损失，限制每一次更新时节点移动距离

过拟合问题：最大最小化损失函数，通过设置有效节点的距离阈值，超过阈值的部分不在优化，缓解过拟合。对于所属类别，优
化方向就是训练点的方向，但是对于其他节点，优化方向是远离所属节点的方向，会不会换成远离训练点会更好？

小样本问题：

思考：特征空间的拓扑结构而不是原型向量的值保存了原有分类信息，也就是说应当尽量保持拓扑结构不变而不是设置锚点。锚
点可以保证原有类的结构不变但不能保证新加入类别以后整体仍然相似，所以类别增多性能下降比较严重（有可能，我猜的）


CEC：
Few-Shot Incremental Learning with Continually Evolved Classifiers

提出了一个三阶段网络：1正常对D0做监督学习，2提出GAT注意力结构，用元学习方式训练（还是在D0上）对过往的所有原型向
量都进行注意力计算，3在新数据集上学一个分类器（全连接）

遗忘问题：将特征提取层冻结

过拟合问题：将新的特征视作旧特征的组合，通过自注意力进行加权。这边说的的新旧特征包含了每一轮学到的特征，也就是说它
有一个上下文的关系，可以考虑到随着训练轮次增加不同特征之间的关系。通过元学习学习这种加权方式，学习如何由旧特征组和
得到新特征

小样本问题：参数比较少，只要训练一个分类器就可以

FeSSSS
Few-Shot Class Incremental Learning Leveraging Self-Supervised Features

做了特征融合，既然小样本的难点在于数据不足，那么就引入自监督模型，通过数据融合涨点。主要有意思的点在于，这篇文章证
明了，采用softmax方式也可以做增量学习，只要把原有线性层的参数复制过去就可以。

遗忘问题：

过拟合问题：

小样本：

FACT
Forward Compatible Few-Shot Class-Incremental Learning

提出了一种想法，训练时可以提前为未来的类别扩展做准备，具体方法是假定存在若干个虚拟类，训练base类时目标是双峰分布，
增量时新的类别的原型向量根据虚拟类别组和得到。个人理解，此方法暗含了一种假设：新旧类别所需要的特征具有相似性，这样
可以在训练base类时提前为新的类做准备。

遗忘问题：

过拟合问题：

小样本：新向量由旧有向量得到









